---
title: "Applied Statistical Programming - The EM Algorithm"
date: "4/13/2022"
header-includes:
- \usepackage{amsmath}
- \usepackage{geometry}
- \usepackage{hyperref}
- \usepackage{setspace}
- \usepackage{hyperref}
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{Write R and Rcpp code to answer the following questions. Write the code, and then show what the computer returns when that code is run. Thoroughly comment your solutions.}

Complete this assignment before 10:00am on Wednesday, April 20. Submit the R implementation as an Rmarkdown and the knitted PDF to Canvas. Have one group member submit the activity with all group members listed at the top. The Rcpp portion will be given to you as your final assignment.


\section*{In-class Background: The Expectation-Maximization Algorithm}

The goal of this in-class exercise is to implement an ensemble of models. You will combine forecasts of US presidential elections using ensemble Bayesian model averaging (EBMA). To do this, you must decide how to weight each component of the forecast in the prediction. The collection of these weighted forecasts form the ensemble, and you will use something called the EM (expectation-maximization) algorithm.

The task is to choose values $w_k$ that maximize the following equation:

\begin{equation}
   p(y \vert f_1^{s \vert t^{\star}}, ..., f_K^{s \vert t^{\star}}) = \sum\limits^N_{k=1} w_k N(f_k^{t^{\star}}, \sigma^{2})
\end{equation}

For the remainder of this assignment, assume that the parameter $\sigma^{2}$ is known and that $\sigma^{2} = 1$. 

The first step of the EM algorithm is to estimate the latent quantity $\hat{z}^t_k$ that represents the probability that observation $t$ was best predicted by model $k$.

\begin{equation}
   \hat{z}_k^{(j+1)t} = \frac{\hat{w}_k^{(j)} N(y^t \vert f_k^t, 1)}{\sum\limits^N_{k=1} \hat{w}_k^{(j)} N(y^t \vert f_k^t, 1)}
\end{equation}

In this equation, $j$ is the particular iteration of the EM algorithm, and $N(y^t \vert f_k^t, 1)$ is the normal cumulative distribution function evaluated at the observed election outcome (\texttt{dnorm(y, ftk, 1)}).

The second step of the EM algorithm is to estimate the expected value of the weights assuming that all $\hat{z}^{t}_{k}$ are correct.

\begin{equation}
   \hat{w}^{(j+1)}_k = \frac{1}{n} \sum_t \hat{z}_k^{(j+1)t}
\end{equation}

\newpage

The estimation procedure is as follows:
\begin{enumerate}
   \item Start with the assumption that all models are weighted equally.
   \item Calculate $\hat{z}_k^{(j+1)t}$ for each model for each election.
   \item Calculate $\hat{w}_k^{(j+1)}$ for each model.
   \item Repeat steps 2-3 twenty times.
\end{enumerate}

Complete the preceding tasks in \texttt{R} alone.

**Answer**:

```{r}
## Logic:
### Step 1: Set up the loop for iterating the calculation of weights (length iter). 
###         Go to the first iteration where the initial weights are defined. 
###         Set up an empty zsk matrix to store values of zsk.

### Step 2: Set up the loop for y values (length n), Take in the first value of y of length n, say y1.
###         Set up an empty numerator vector to store values of numerators (the second equation). 

### Step 3: Set up the loop for ftk values (length m). 
###         Place y1 to the ftk loop to calculate each numerator associated with each ftk.
###         Store the calculated numerator to the vector named num_vec.

### Step 4: num_vec / sum(num_vec) returns a ztk vector of length m. Save this vector to the row of the empty zsk matrix.

### Step 5: Go back to Step 2 and take the next value of y. Repeat this process for all y values. 

### Step 6, Now we have a zsk matrix. 
###         Each row of the matrix stores the ztk values corresponding to each y value (hence, aross all ftk values).
###         Each column of the matrix stores the ztk values corresponding to each ftk values (hence, across all y values). 

### Step 7: colMeans(zsk_mat) returns the new weight for each ftk.

### Step 8: Feed the new weights to the next iteration. That means we go back to Step 1.

### Step 9: Go through all the iterations and the final weights will be calculated. 

est_weights <- function(y, ftk, weights=rep(1/length(ftk),length(ftk)), sd=1, iter=20) {
  n <- length(y) 
  m <- length(ftk)
  
  for (k in 1:iter) {
    zsk_mat <- matrix(nrow = n, ncol = m) 
    
    for (i in 1:n) {
      num_vec <- c() 
      
      for (j in 1:m) {
        num <- dnorm(y[i],ftk[j],sd) * weights[j]
        num_vec <- append(num_vec,num)
      }
      
      den <- sum(num_vec)
      ztk <- num_vec / den
      zsk_mat[i,] <- ztk
    }

    weights <- colMeans(zsk_mat)
  }
  return(weights)
} 

y <- c(0.2,0.5)
ftk <- seq(0,1,0.001)
sd <- 1
weights <- rep(1/length(ftk), length(ftk))
est_weights(y=y, ftk=ftk)
sum(est_weights(y=y, ftk=ftk))

```

```{r}
library(profvis)
profvis(est_weights(y=y, ftk=ftk))
```


\section*{Assignment: Rcpp Practice}


\begin{enumerate}
   \item Write an Rcpp function that will calculate the answer to Equation (2). The output will be a matrix.
   \item Write an Rcpp function that will calculate the answer to Equation (3). The output will be a vector.
   \item Write an Rcpp function that will complete the entire algorithm.
\end{enumerate}
